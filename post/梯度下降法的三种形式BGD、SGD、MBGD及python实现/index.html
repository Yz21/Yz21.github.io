<html>
  <head>
    <meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>梯度下降法的三种形式BGD、SGD、MBGD及python实现 | Gridea</title>
<meta name="description" content="温故而知新" />
<link rel="shortcut icon" href="https://Yz21.github.io//favicon.ico?v=1565658376063">
<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.7.2/css/all.css" integrity="sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css">
<link rel="stylesheet" href="https://Yz21.github.io//styles/main.css">

<script src="https://cdn.bootcss.com/highlight.js/9.12.0/highlight.min.js"></script>
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Droid+Serif:400,700">



  </head>
  <body>
    <div class="main">
      <div class="main-content">
        <div class="site-header">
  <a href="https://Yz21.github.io/">
  <img class="avatar" src="https://Yz21.github.io//images/avatar.png?v=1565658376063" alt="">
  </a>
  <h1 class="site-title">
    Gridea
  </h1>
  <p class="site-description">
    温故而知新
  </p>
  <div class="menu-container">
    
      
        <a href="/" class="menu">
          首页
        </a>
      
    
      
        <a href="/archives" class="menu">
          归档
        </a>
      
    
      
        <a href="/tags" class="menu">
          标签
        </a>
      
    
      
        <a href="/post/about" class="menu">
          关于
        </a>
      
    
  </div>
  <div class="social-container">
    
      
    
      
    
      
    
      
    
      
    
  </div>
</div>

        <div class="post-detail">
          <article class="post">
            <h2 class="post-title">
              梯度下降法的三种形式BGD、SGD、MBGD及python实现
            </h2>
            <div class="post-info">
              <span>
                2019-07-18
              </span>
              <span>
                3 min read
              </span>
              
                <a href="https://Yz21.github.io//tag/z68jcmpyW" class="post-tag">
                  # 机器学习
                </a>
              
                <a href="https://Yz21.github.io//tag/EWffYRyHc4" class="post-tag">
                  # python
                </a>
              
                <a href="https://Yz21.github.io//tag/wHpkUo6WMC" class="post-tag">
                  # 梯度下降
                </a>
              
            </div>
            
            <div class="post-content-wrapper">
              <div class="post-content">
                <meta name="referrer" content="no-referrer" />
<h1 id="前言">前言</h1>
<p>        梯度下降法作为机器学习中较常使用的优化算法，其有着三种不同的形式：批量梯度下降（Batch Gradient Descent）、随机梯度下降（Stochastic Gradient Descent）以及小批量梯度下降（Mini-Batch Gradient Descent）。其中小批量梯度下降法也常用在深度学习中进行模型的训练。接下来，我们将对这三种不同的梯度下降法进行理解。</p>
<!-- more -->
<p>为了便于理解，这里我们将使用只含有一个特征的线性回归来展开。</p>
<p>此时线性回归的假设函数为：<br>
<img src="https://img-blog.csdnimg.cn/20190725100634816.png" alt=""><br>
对应的**目标函数（代价函数）**即为：<br>
<img src="https://img-blog.csdnimg.cn/20190725085658136.png" alt=""><br>
下图为 J(θ0,θ1)与参数 θ0,θ1 的关系的图：<br>
<img src="https://img-blog.csdnimg.cn/20190725090156368.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzQxMzEy,size_16,color_FFFFFF,t_70" alt=""></p>
<h2 id="1-批量梯度下降batch-gradient-descentbgd">1、批量梯度下降（Batch Gradient Descent，BGD）</h2>
<p>        <strong>批量梯度下降法</strong>是最原始的形式，它是指在<strong>每一次迭代时</strong>使用<strong>所有样本</strong>来进行梯度的更新。从数学上理解如下：<br>
  （1）对目标函数求偏导：<br>
  <img src="https://img-blog.csdnimg.cn/20190725085839458.png" alt=""><br>
  其中 i=1,2,...,m 表示样本数， j=0,1 表示特征数，这里我们使用了偏置项 x(i)0=1。<br>
  （2）每次迭代对参数进行更新：<br>
  <img src="https://img-blog.csdnimg.cn/2019072508594192.png" alt=""><br>
        <strong>注意这里更新时存在一个求和函数，即为对所有样本进行计算处理，可与下文SGD法进行比较。</strong><br>
  伪代码形式为：<br>
  <img src="https://img-blog.csdnimg.cn/201907250859587.png" alt=""><br>
  <strong>优点</strong>：<br>
  （1）一次迭代是对所有样本进行计算，此时利用矩阵进行操作，实现了并行。<br>
  （2）由全数据集确定的方向能够更好地代表样本总体，从而更准确地朝向极值所在的方向。当目标函数为凸函数时，BGD一定能够得到全局最优。<br>
  <strong>缺点</strong>：<br>
  （1）当样本数目 mm 很大时，每迭代一步都需要对所有样本计算，训练过程会很慢。<br>
  从迭代的次数上来看，BGD迭代的次数相对较少。其迭代的收敛曲线示意图可以表示如下：<br>
  <img src="https://img-blog.csdnimg.cn/20190725090052713.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzQxMzEy,size_16,color_FFFFFF,t_70" alt=""></p>
<h2 id="批量梯度下降法的python实现">批量梯度下降法的python实现</h2>
<pre><code>import matplotlib.pyplot as plt
import random
##样本数据
x_train = [150,200,250,300,350,400,600]
y_train = [6450,7450,8450,9450,11450,15450,18450]
#样本个数
m = len(x_train)
#步长
alpha = 0.00001
#循环次数
cnt = 0
#假设函数为 y=theta0+theta1*x
def h(x):
    return theta0 + theta1*x
theta0 = 0
theta1 = 0
#导数
diff0=0
diff1=0
#误差
error0=0           
error1=0          
#每次迭代theta的值
retn0 = []         
retn1 = []         

#退出迭代的条件
epsilon=0.00001

#批量梯度下降
while 1:
    cnt=cnt+1
    diff0=0
    diff1=0
    #梯度下降
    for i in range(m):
        diff0+=h(x_train[i])-y_train[i]
        diff1+=(h(x_train[i])-y_train[i])*x_train[i]
    theta0=theta0-alpha/m*diff0
    theta1=theta1-alpha/m*diff1
    retn0.append(theta0)
    retn1.append(theta1)
    error1=0
    #计算迭代误差
    for i in range(len(x_train)):
        error1 += ((theta0 + theta1 * x_train[i])-y_train[i]) ** 2 / 2
    #判断是否已收敛
    if abs(error1 - error0) &lt; epsilon:
        break
    else:
        error0 = error1
# 画图表现
plt.title('BGD')
plt.plot(range(len(retn0)),retn0,label='theta0')
plt.plot(range(len(retn1)),retn1,label='theta1')
plt.legend()          #显示上面的label
plt.xlabel('time')
plt.ylabel('theta')
plt.show()
plt.plot(x_train,y_train,'bo')
plt.plot(x_train,[h(x) for x in x_train],color='k',label='BGD')
plt.legend()
plt.xlabel('area')
plt.ylabel('price')
print(&quot;批量梯度下降法：theta0={},theta1={}&quot;.format(theta0,theta1))
print(&quot;批量梯度下降法循环次数：{}&quot;.format(cnt))
plt.show()
</code></pre>
<h2 id="2-随机梯度下降stochastic-gradient-descentsgd">2、随机梯度下降（Stochastic Gradient Descent，SGD）</h2>
<p>       &amp;nbsp**;随机梯度下降法<strong>不同于批量梯度下降，随机梯度下降是</strong>每次迭代<strong>使用</strong>一个样本**来对参数进行更新。使得训练速度加快。</p>
<p>        对于一个样本的目标函数为：<br>
  <img src="https://img-blog.csdnimg.cn/20190725090216938.png" alt=""><br>
  （1）对目标函数求偏导：<br>
  <img src="https://img-blog.csdnimg.cn/20190725090246509.png" alt=""><br>
  （2）参数更新：<br>
  <img src="https://img-blog.csdnimg.cn/20190725090315940.png" alt=""><br>
  <strong>注意，这里不再有求和符号</strong><br>
  伪代码形式为：<br>
  <img src="https://img-blog.csdnimg.cn/20190725090345289.png" alt=""><br>
        <strong>优点</strong>：<br>
  （1）由于不是在全部训练数据上的损失函数，而是在每轮迭代中，随机优化某一条训练数据上的损失函数，这样每一轮参数的更新速度大大加快。<br>
        <strong>缺点</strong>：<br>
  （1）准确度下降。由于即使在目标函数为强凸函数的情况下，SGD仍旧无法做到线性收敛。<br>
  （2）可能会收敛到局部最优，由于单个样本并不能代表全体样本的趋势。<br>
  （3）不易于并行实现。</p>
<p>        <strong>解释一下为什么SGD收敛速度比BGD要快：</strong><br>
                答：这里我们假设有30W个样本，对于BGD而言，每次迭代需要计算30W个样本才能对参数进行一次更新，需要求得最小值可能需要多次迭代（假设这里是10）；而对于SGD，每次更新参数只需要一个样本，因此若使用这30W个样本进行参数更新，则参数会被更新（迭代）30W次，而这期间，SGD就能保证能够收敛到一个合适的最小值上了。也就是说，在收敛时，BGD计算了 10×30W 次，而SGD只计算了 1×30W 次。</p>
<p>        从迭代的次数上来看，SGD迭代的次数较少，在解空间的搜索过程看起来很盲目。其迭代的收敛曲线示意图可以表示如下：<br>
  <img src="https://img-blog.csdnimg.cn/20190725090459507.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzQxMzEy,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="随机梯度下降法的python实现">随机梯度下降法的python实现</h2>
<pre><code>import matplotlib.pyplot as plt
import random
##样本数据
x_train = [150,200,250,300,350,400,600]
y_train = [6450,7450,8450,9450,11450,15450,18450]
#样本个数
m = len(x_train)
#步长
alpha = 0.00001
#循环次数
cnt = 0
#假设函数为 y=theta0+theta1*x
def h(x):
    return theta0 + theta1*x
theta0 = 0
theta1 = 0
#导数
diff0=0
diff1=0
#误差
error0=0           
error1=0          
#每次迭代theta的值
retn0 = []         
retn1 = []         

#退出迭代的条件
epsilon=0.00001

#随机梯度下降
for i in range(1000):
    cnt=cnt+1
    diff0=0
    diff1=0
    j = random.randint(0, m - 1)
    diff0=h(x_train[j])-y_train[j]
    diff1=(h(x_train[j])-y_train[j])*x_train[j]
    theta0=theta0-alpha/m*diff0
    theta1=theta1-alpha/m*diff1
    retn0.append(theta0)
    retn1.append(theta1)
    error1=0
    #计算迭代的误差
    for i in range(len(x_train)):
        error1 += ((theta0 + theta1 * x_train[i])-y_train[i]) ** 2 / 2
    #判断是否已收敛
    if abs(error1 - error0) &lt; epsilon:
        break
    else:
        error0 = error1
# 画图表现        
plt.title('SGD')
plt.plot(range(len(retn0)),retn0,label='theta0')
plt.plot(range(len(retn1)),retn1,label='theta1')
plt.legend()          #显示上面的label
plt.xlabel('time')
plt.ylabel('theta')
plt.show()
plt.plot(x_train,y_train,'bo')
plt.plot(x_train,[h(x) for x in x_train],color='k',label='SGD')
plt.legend()
plt.xlabel('area')
plt.ylabel('price')
print(&quot;随机梯度下降法：theta0={},theta1={}&quot;.format(theta0,theta1))
print(&quot;随机梯度下降法循环次数：{}&quot;.format(cnt))
plt.show()
</code></pre>
<h2 id="3-小批量梯度下降mini-batch-gradient-descent-mbgd">3、小批量梯度下降（Mini-Batch Gradient Descent, MBGD）</h2>
<p>        <strong>小批量梯度下降</strong>，是对批量梯度下降以及随机梯度下降的一个折中办法。其思想是：<strong>每次迭代</strong> 使用 <strong>batch_size 个样本</strong>来对参数进行更新。<br>
        这里我们假设 batchsize=10，样本数 m=1000 。<br>
        伪代码形式为：<br>
  <img src="https://img-blog.csdnimg.cn/20190725090543904.png" alt=""><br>
  <strong>优点</strong>：<br>
  （1）通过矩阵运算，每次在一个batch上优化神经网络参数并不会比单个数据慢太多。<br>
  （2）每次使用一个batch可以大大减小收敛所需要的迭代次数，同时可以使收敛到的结果更加接近梯度下降的效果。(比如上例中的30W，设置batch_size=100时，需要迭代3000次，远小于SGD的30W次)<br>
  （3）可实现并行化。<br>
  <strong>缺点</strong>：<br>
  （1）batch_size的不当选择可能会带来一些问题。</p>
<p>        <strong>batcha_size的选择带来的影响：</strong><br>
  （1）在合理地范围内，增大batch_size的好处：<br>
    a. 内存利用率提高了，大矩阵乘法的并行化效率提高。<br>
    b. 跑完一次 epoch（全数据集）所需的迭代次数减少，对于相同数据量的处理速度进一步加快。<br>
    c. 在一定范围内，一般来说 Batch_Size 越大，其确定的下降方向越准，引起训练震荡越小。<br>
  （2）盲目增大batch_size的坏处：<br>
    a. 内存利用率提高了，但是内存容量可能撑不住了。<br>
    b. 跑完一次 epoch（全数据集）所需的迭代次数减少，要想达到相同的精度，其所花费的时间大大增加了，从而对参数的修正也就显得更加缓慢。<br>
    c. Batch_Size 增大到一定程度，其确定的下降方向已经基本不再变化。</p>
<p>        下图显示了三种梯度下降算法的收敛过程：<br>
  <img src="https://img-blog.csdnimg.cn/20190725090635209.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzQxMzEy,size_16,color_FFFFFF,t_70" alt=""></p>
<h2 id="4-总结">4、总结</h2>
<p><strong>Batch gradient descent:</strong> Use all examples in each iteration；</p>
<p><strong>Stochastic gradient descent:</strong> Use 1 example in each iteration；</p>
<p><strong>Mini-batch gradient descent:</strong> Use b examples in each iteration.</p>

              </div>
              <div class="toc-container">
                <ul class="markdownIt-TOC">
<li><a href="#%E5%89%8D%E8%A8%80">前言</a>
<ul>
<li><a href="#1-%E6%89%B9%E9%87%8F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8Dbatch-gradient-descentbgd">1、批量梯度下降（Batch Gradient Descent，BGD）</a></li>
<li><a href="#%E6%89%B9%E9%87%8F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E7%9A%84python%E5%AE%9E%E7%8E%B0">批量梯度下降法的python实现</a></li>
<li><a href="#2-%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8Dstochastic-gradient-descentsgd">2、随机梯度下降（Stochastic Gradient Descent，SGD）</a></li>
<li><a href="#%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E7%9A%84python%E5%AE%9E%E7%8E%B0">随机梯度下降法的python实现</a></li>
<li><a href="#3-%E5%B0%8F%E6%89%B9%E9%87%8F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8Dmini-batch-gradient-descent-mbgd">3、小批量梯度下降（Mini-Batch Gradient Descent, MBGD）</a></li>
<li><a href="#4-%E6%80%BB%E7%BB%93">4、总结</a></li>
</ul>
</li>
</ul>

              </div>
            </div>
          </article>
        </div>

        
          <div class="next-post">
            <div class="next">下一篇</div>
            <a href="https://Yz21.github.io//post/逻辑演算">
              <h3 class="post-title">
                python+离散数学→逻辑演算
              </h3>
            </a>
          </div>
        

        

        <div class="site-footer">
  Powered by <a href="https://github.com/getgridea/gridea" target="_blank">Gridea</a> | 
  <a class="rss" href="https://Yz21.github.io//atom.xml" target="_blank">RSS</a>
</div>

<script>
  hljs.initHighlightingOnLoad()

  let mainNavLinks = document.querySelectorAll(".markdownIt-TOC a");

  // This should probably be throttled.
  // Especially because it triggers during smooth scrolling.
  // https://lodash.com/docs/4.17.10#throttle
  // You could do like...
  // window.addEventListener("scroll", () => {
  //    _.throttle(doThatStuff, 100);
  // });
  // Only not doing it here to keep this Pen dependency-free.

  window.addEventListener("scroll", event => {
    let fromTop = window.scrollY;

    mainNavLinks.forEach((link, index) => {
      let section = document.getElementById(decodeURI(link.hash).substring(1));
      let nextSection = null
      if (mainNavLinks[index + 1]) {
        nextSection = document.getElementById(decodeURI(mainNavLinks[index + 1].hash).substring(1));
      }
      console.log('section.offsetHeight', section.offsetHeight);
      if (section.offsetTop <= fromTop) {
        if (nextSection) {
          if (nextSection.offsetTop > fromTop) {
            link.classList.add("current");
          } else {
            link.classList.remove("current");    
          }
        } else {
          link.classList.add("current");
        }
      } else {
        link.classList.remove("current");
      }
    });
  });

</script>

      </div>
    </div>
  </body>
</html>
