<html>
  <head>
    <meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>人工神经网络（ANN） | Gridea</title>
<meta name="description" content="温故而知新" />
<link rel="shortcut icon" href="https://Yz21.github.io//favicon.ico?v=1565658376063">
<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.7.2/css/all.css" integrity="sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css">
<link rel="stylesheet" href="https://Yz21.github.io//styles/main.css">

<script src="https://cdn.bootcss.com/highlight.js/9.12.0/highlight.min.js"></script>
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Droid+Serif:400,700">



  </head>
  <body>
    <div class="main">
      <div class="main-content">
        <div class="site-header">
  <a href="https://Yz21.github.io/">
  <img class="avatar" src="https://Yz21.github.io//images/avatar.png?v=1565658376063" alt="">
  </a>
  <h1 class="site-title">
    Gridea
  </h1>
  <p class="site-description">
    温故而知新
  </p>
  <div class="menu-container">
    
      
        <a href="/" class="menu">
          首页
        </a>
      
    
      
        <a href="/archives" class="menu">
          归档
        </a>
      
    
      
        <a href="/tags" class="menu">
          标签
        </a>
      
    
      
        <a href="/post/about" class="menu">
          关于
        </a>
      
    
  </div>
  <div class="social-container">
    
      
    
      
    
      
    
      
    
      
    
  </div>
</div>

        <div class="post-detail">
          <article class="post">
            <h2 class="post-title">
              人工神经网络（ANN）
            </h2>
            <div class="post-info">
              <span>
                2019-07-26
              </span>
              <span>
                2 min read
              </span>
              
                <a href="https://Yz21.github.io//tag/z68jcmpyW" class="post-tag">
                  # 机器学习
                </a>
              
                <a href="https://Yz21.github.io//tag/H02Q169PcM" class="post-tag">
                  # BP算法
                </a>
              
                <a href="https://Yz21.github.io//tag/aFulsB5AMO" class="post-tag">
                  # 人工神经网络（ANN）
                </a>
              
            </div>
            
            <div class="post-content-wrapper">
              <div class="post-content">
                <h1 id="前言">前言</h1>
<p>        初学人工智能不久，今天碰上了人工神经网（ANN），开始学的时候很懵，一大堆理论、公式、推导.....作为一名小白，还是很痛苦的，不过经过摸索，大概了 解了什么是ANN，公式的推导以及一些其他问题，下面我就总结下自己的理解，一方面作为自己的笔记，日后方便巩固；另一方面，也可以分享给其他有意者。</p>
<!-- more -->
<h1 id="一-什么是神经网络">一、什么是神经网络</h1>
<h2 id="1单层神经网络">1.单层神经网络</h2>
<p>首先以单层神经元为例解释人工神经元是如何工作的<br>
<img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9zdGF0aWMubGVpcGhvbmUuY29tL3VwbG9hZHMvbmV3L2FydGljbGUvNzQwXzc0MC8yMDE2MDgvNTdiZDUyZjMwMjYzZC5wbmc" alt=""><br>
        x1,x2,…, xN：神经元的输入。这些可以从输入层实际观测或者是一个隐藏层的中间值（隐藏层即介于输入与输出之间的所有节点组成的一层。后面讲到多层神经网络是会再跟大家解释的）。</p>
<p>        X0：偏置单元。这是常值添加到激活函数的输入（类似数学里y＝ax+b中使直线不过原点的常数b）。即截距项，通常有＋1值。</p>
<p>        w0,w1,w2,…,wN：对应每个输入的权重。甚至偏置单元也是有权重的。</p>
<p>a:神经元的输出。计算如下：<br>
<img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9zdGF0aWMubGVpcGhvbmUuY29tL3VwbG9hZHMvbmV3L2FydGljbGUvNzQwXzc0MC8yMDE2MDgvNTdiZDUyZjQ2NzM2Zi5wbmc" alt=""><br>
        式子里的f是已知的激活函数，f使神经网络（单层乃至多层）非常灵活并且具有能估计复杂的非线性关系的能力。在简单情况下可以是一个高斯函数、逻辑函数、双曲线函数或者甚至上是一个线性函数。利用神经网络可让其实现三个基本功能：与、或、非（AND, OR, NOT）。</p>
<p>        这里引入一个例子：and功能实现如下<br>
<img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9zdGF0aWMubGVpcGhvbmUuY29tL3VwbG9hZHMvbmV3L2FydGljbGUvNzQwXzc0MC8yMDE2MDgvNTdiZDUyZjc3YjA2Yy5wbmc" alt=""><br>
神经元输出：a = f( -1.5 + x1 + x2 )</p>
<p>这样大家就很容易理解其工作原理，其实就是对输入值赋予不同权重，经过激活函数输出的过程。</p>
<h2 id="2多层神经网络">2.多层神经网络</h2>
<h3 id="21-网络结构">2.1 网络结构</h3>
<p>清楚了单层神经网络，多层神经网络也好理解了，就相当于多个单层的叠加成多层的过程。<br>
<img src="https://img-blog.csdnimg.cn/20190811204232448.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzQxMzEy,size_16,color_FFFFFF,t_70" alt=""><br>
<strong>神经网络分为三种类型的层</strong>：</p>
<p>输入层：神经网络最左边的一层，通过这些神经元输入需要训练观察的样本，即初始输入数据的一层。</p>
<p>隐藏层：介于输入与输出之间的所有节点组成的一层。帮助神经网络学习数据间的复杂关系，即对数据进行处理的层。</p>
<p>输出层：由前两层得到神经网络最后一层，即最后结果输出的一层。</p>
<h3 id="22-传递函数激活函数">2.2 传递函数/激活函数</h3>
<p>        前面每一层输入经过线性变换wx+b后还用到了sigmoid函数，在神经网络的结构中被称为传递函数或者激活函数。除了sigmoid，还有tanh、relu等别的激活函数。激活函数使线性的结果非线性化。</p>
<h3 id="23-为什么需要传递函数">2.3 为什么需要传递函数</h3>
<p>        简单理解上，如果不加激活函数，无论多少层隐层，最终的结果还是原始输入的线性变化，这样一层隐层就可以达到结果，就没有多层感知器的意义了。所以每个隐层都会配一个激活函数，提供非线性变化。</p>
<h1 id="二-bp算法">二、BP算法</h1>
<h2 id="1bp算法基本思想">1.BP算法基本思想</h2>
<p>        BP算法全称叫作误差反向传播(error Back Propagation，或者也叫作误差逆传播)算法。其算法基本思想为：在上述的前馈网络中，输入信号经输入层输入，通过隐层计算由输出层输出，输出值与标记值比较，若有误差，将误差反向由输出层向输入层传播，在这个过程中，利用梯度下降算法对神经元权值进行调整。</p>
<h2 id="2bp算法的推导">2.BP算法的推导</h2>
<h3 id="21-数学基础理论">2.1 数学基础理论</h3>
<p>        BP算法中核心的数学工具就是微积分的<strong>链式求导法则</strong>。<br>
<img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xMjQxMzk3LWMwN2Q1ODFjOTJkYWViZWMucG5n" alt=""></p>
<h3 id="22推导过程">2.2推导过程</h3>
<p><img src="https://img-blog.csdnimg.cn/20190811205148228.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzQxMzEy,size_16,color_FFFFFF,t_70" alt=""></p>
<ol>
<li><strong>正向传播求损失，反向传播回传误差</strong></li>
<li>根据误差信号修正每层的权重</li>
<li>f是激活函数；f(netj)是隐层的输出； f(netk）是输出层的输出O; d是target</li>
</ol>
<p>        结合BP网络结构，误差由输出展开至输入的过程如下：<br>
<img src="https://img-blog.csdnimg.cn/20190811205328707.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzQxMzEy,size_16,color_FFFFFF,t_70" alt=""><br>
有了误差E，通过求偏导就可以求得最优的权重。（不要忘记学习率）</p>
<p><img src="https://img-blog.csdnimg.cn/2019081120540142.png" alt=""></p>
<h2 id="3-举例说明">3. 举例说明</h2>
<p><img src="https://img-blog.csdnimg.cn/20190811205641720.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzQxMzEy,size_16,color_FFFFFF,t_70" alt=""><br>
图中元素：<br>
两个输入；<br>
隐层: b1, w1, w2, w3, w4 (都有初始值）<br>
输出层：b2, w5, w6, w7, w8（赋了初始值）</p>
<h3 id="31-前向传播">3.1 前向传播</h3>
<p><img src="https://img-blog.csdnimg.cn/20190811205743162.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzQxMzEy,size_16,color_FFFFFF,t_70" alt=""><br>
则误差：<br>
<img src="https://img-blog.csdnimg.cn/20190811205806851.png" alt=""></p>
<h3 id="32-反向传播">3.2 反向传播</h3>
<p><img src="https://img-blog.csdnimg.cn/20190811205859442.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzQxMzEy,size_16,color_FFFFFF,t_70" alt=""><br>
参数更新：<br>
<img src="https://img-blog.csdnimg.cn/20190811205921617.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzQxMzEy,size_16,color_FFFFFF,t_70" alt=""><br>
求误差对w1的偏导 ：<br>
<img src="https://img-blog.csdnimg.cn/20190811205940706.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzQxMzEy,size_16,color_FFFFFF,t_70" alt=""><br>
注意，w1对两个输出的误差都有影响<br>
通过以上过程可以更新所有权重，就可以再次迭代更新了，直到满足条件。</p>
<h1 id="三-python代码实现">三、python代码实现</h1>
<p>        以上述例子，用python可写出如下代码，并附有详解：</p>
<pre><code>import numpy as np
import matplotlib.pyplot as plt
import math
a=np.array([0.05,0.1])           #a1,a2的输入值
weight1=np.array([[0.15,0.25],[0.2,0.3]])   #a1对b1,b2的权重，a2对b1，b2的权重
weight2=np.array([[0.4,0.5],[0.45,0.55]])     #b1对c1,c2的权重，b2对c1，c2的权重
target=np.array([0.01,0.99])
d1=0.35   #输入层的偏置（1）的权重
d2=0.6    #隐藏层的偏置（1）的权重
β=0.5    #学习效率

#一：前向传播

#计算输入层到隐藏层的输入值，得矩阵netb1,netb2
netb=np.dot(a,weight1)+d1

#计算隐藏层的输出值,得到矩阵outb1,outb2
m=[]
for i in range(len(netb)):
    outb=1.0 / (1.0 + math.exp(-netb[i]))
    m.append(outb)
m=np.array(m)

#计算隐藏层到输出层的输入值，得矩阵netc1,netc2
netc=np.dot(m,weight2)+d2

#计算隐藏层的输出值,得到矩阵outc1,outc2
n=[]
for i in range(len(netc)):
    outc=1.0 / (1.0 + math.exp(-netc[i]))
    n.append(outc)
n=np.array(n)

#二：反向传播
count=0 #计数
e=0     #误差  
E=[]    #统计误差
#梯度下降
while True:
  count+=1
  
  #总误差对w1-w4的偏导
  pd1=(-(target[0]-n[0])*n[0]*(1-n[0])*weight2[0][0]-(target[1]-n[1])*n[1]*(1-n[1])*weight2[0][1])*m[0]*(1-m[0])*a[0]
  pd2=(-(target[0]-n[0])*n[0]*(1-n[0])*weight2[0][0]-(target[1]-n[1])*n[1]*(1-n[1])*weight2[0][1])*m[0]*(1-m[0])*a[1]
  pd3=(-(target[0]-n[0])*n[0]*(1-n[0])*weight2[1][0]-(target[1]-n[1])*n[1]*(1-n[1])*weight2[0][1])*m[0]*(1-m[0])*a[0]                          
  pd4=(-(target[0]-n[0])*n[0]*(1-n[0])*weight2[1][1]-(target[1]-n[1])*n[1]*(1-n[1])*weight2[0][1])*m[0]*(1-m[0])*a[1]
  weight1[0][0]=weight1[0][0]-β*pd1
  weight1[1][0]=weight1[1][0]-β*pd2
  weight1[0][1]=weight1[0][1]-β*pd3
  weight1[1][1]=weight1[1][1]-β*pd4

  #总误差对w5-w8的偏导
  pd5=-(target[0]-n[0])*n[0]*(1-n[0])*m[0]
  pd6=-(target[0]-n[0])*n[0]*(1-n[0])*m[1]
  pd7=-(target[1]-n[1])*n[1]*(1-n[1])*m[0]
  pd8=-(target[1]-n[1])*n[1]*(1-n[1])*m[1]
  weight2[0][0]=weight2[0][0]-β*pd5
  weight2[1][0]=weight2[1][0]-β*pd6
  weight2[0][1]=weight2[0][1]-β*pd7
  weight2[1][1]=weight2[1][1]-β*pd8
  
  netb=np.dot(a,weight1)+d1
  m=[]
  for i in range(len(netb)):
    outb=1.0 / (1.0 + math.exp(-netb[i]))
    m.append(outb)
  m=np.array(m)
  netc=np.dot(m,weight2)+d2
  n=[]
  for i in range(len(netc)):
    outc=1.0 / (1.0 + math.exp(-netc[i]))
    n.append(outc)
  n=np.array(n)
  
  #计算总误差
  for j in range(len(n)):
    e += (target[j]-n[j])**2/2
  E.append(e)
  #判断
  if e&lt;0.0000001:
    break
  else:
      e=0
print(count)
print(e)
print(n)
plt.plot(range(len(E)),E,label='error')
plt.legend() 
plt.xlabel('time')
plt.ylabel('error')
plt.show()
</code></pre>
<h1 id="四-bp神经网络的优缺点">四、BP神经网络的优缺点</h1>
<p><strong>BP神经网络的优点：</strong></p>
<ol>
<li>非线性映射能力</li>
<li>泛化能力</li>
<li>容错能力，允许输入样本中带有较大误差甚至个别错误。反应正确规律的知识来自全体样本，个别样本中的误差不能左右对权矩阵的调整</li>
</ol>
<p><strong>BP神经网络的缺陷：</strong></p>
<ol>
<li>需要的参数过多，而且参数的选择没有有效的方法。确定一个BP神经网络需要知道：网络的层数、每一层神经元的个数和权值。权值可以通过学习得到，如果，隐层神经元数量太多会引起过学习，如果隐层神经元个数太少会引起欠学习。此外学习率的选择也是需要考虑。目前来说，对于参数的确定缺少一个简单有效的方法，所以导致算法很不稳定；</li>
<li>属于监督学习，对于样本有较大依赖性，网络学习的逼近和推广能力与样本有很大关系，如果样本集合代表性差，样本矛盾多，存在冗余样本，网络就很难达到预期的性能；</li>
<li>由于权值是随机给定的，所以BP神经网络具有不可重现性；</li>
</ol>

              </div>
              <div class="toc-container">
                <ul class="markdownIt-TOC">
<li><a href="#%E5%89%8D%E8%A8%80">前言</a></li>
<li><a href="#%E4%B8%80-%E4%BB%80%E4%B9%88%E6%98%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C">一、什么是神经网络</a>
<ul>
<li><a href="#1%E5%8D%95%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C">1.单层神经网络</a></li>
<li><a href="#2%E5%A4%9A%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C">2.多层神经网络</a>
<ul>
<li><a href="#21-%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84">2.1 网络结构</a></li>
<li><a href="#22-%E4%BC%A0%E9%80%92%E5%87%BD%E6%95%B0%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0">2.2 传递函数/激活函数</a></li>
<li><a href="#23-%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81%E4%BC%A0%E9%80%92%E5%87%BD%E6%95%B0">2.3 为什么需要传递函数</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#%E4%BA%8C-bp%E7%AE%97%E6%B3%95">二、BP算法</a>
<ul>
<li><a href="#1bp%E7%AE%97%E6%B3%95%E5%9F%BA%E6%9C%AC%E6%80%9D%E6%83%B3">1.BP算法基本思想</a></li>
<li><a href="#2bp%E7%AE%97%E6%B3%95%E7%9A%84%E6%8E%A8%E5%AF%BC">2.BP算法的推导</a>
<ul>
<li><a href="#21-%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E7%90%86%E8%AE%BA">2.1 数学基础理论</a></li>
<li><a href="#22%E6%8E%A8%E5%AF%BC%E8%BF%87%E7%A8%8B">2.2推导过程</a></li>
</ul>
</li>
<li><a href="#3-%E4%B8%BE%E4%BE%8B%E8%AF%B4%E6%98%8E">3. 举例说明</a>
<ul>
<li><a href="#31-%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD">3.1 前向传播</a></li>
<li><a href="#32-%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD">3.2 反向传播</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#%E4%B8%89-python%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0">三、python代码实现</a></li>
<li><a href="#%E5%9B%9B-bp%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E4%BC%98%E7%BC%BA%E7%82%B9">四、BP神经网络的优缺点</a></li>
</ul>

              </div>
            </div>
          </article>
        </div>

        
          <div class="next-post">
            <div class="next">下一篇</div>
            <a href="https://Yz21.github.io//post/多种相似度计算的python实现">
              <h3 class="post-title">
                多种相似度计算的python实现
              </h3>
            </a>
          </div>
        

        

        <div class="site-footer">
  Powered by <a href="https://github.com/getgridea/gridea" target="_blank">Gridea</a> | 
  <a class="rss" href="https://Yz21.github.io//atom.xml" target="_blank">RSS</a>
</div>

<script>
  hljs.initHighlightingOnLoad()

  let mainNavLinks = document.querySelectorAll(".markdownIt-TOC a");

  // This should probably be throttled.
  // Especially because it triggers during smooth scrolling.
  // https://lodash.com/docs/4.17.10#throttle
  // You could do like...
  // window.addEventListener("scroll", () => {
  //    _.throttle(doThatStuff, 100);
  // });
  // Only not doing it here to keep this Pen dependency-free.

  window.addEventListener("scroll", event => {
    let fromTop = window.scrollY;

    mainNavLinks.forEach((link, index) => {
      let section = document.getElementById(decodeURI(link.hash).substring(1));
      let nextSection = null
      if (mainNavLinks[index + 1]) {
        nextSection = document.getElementById(decodeURI(mainNavLinks[index + 1].hash).substring(1));
      }
      console.log('section.offsetHeight', section.offsetHeight);
      if (section.offsetTop <= fromTop) {
        if (nextSection) {
          if (nextSection.offsetTop > fromTop) {
            link.classList.add("current");
          } else {
            link.classList.remove("current");    
          }
        } else {
          link.classList.add("current");
        }
      } else {
        link.classList.remove("current");
      }
    });
  });

</script>

      </div>
    </div>
  </body>
</html>
