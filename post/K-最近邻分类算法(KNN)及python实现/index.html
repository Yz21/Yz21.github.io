<html>
  <head>
    <meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>KNN算法及python实现 | Gridea</title>
<meta name="description" content="温故而知新" />
<link rel="shortcut icon" href="https://Yz21.github.io//favicon.ico?v=1565658376063">
<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.7.2/css/all.css" integrity="sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css">
<link rel="stylesheet" href="https://Yz21.github.io//styles/main.css">

<script src="https://cdn.bootcss.com/highlight.js/9.12.0/highlight.min.js"></script>
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Droid+Serif:400,700">



  </head>
  <body>
    <div class="main">
      <div class="main-content">
        <div class="site-header">
  <a href="https://Yz21.github.io/">
  <img class="avatar" src="https://Yz21.github.io//images/avatar.png?v=1565658376063" alt="">
  </a>
  <h1 class="site-title">
    Gridea
  </h1>
  <p class="site-description">
    温故而知新
  </p>
  <div class="menu-container">
    
      
        <a href="/" class="menu">
          首页
        </a>
      
    
      
        <a href="/archives" class="menu">
          归档
        </a>
      
    
      
        <a href="/tags" class="menu">
          标签
        </a>
      
    
      
        <a href="/post/about" class="menu">
          关于
        </a>
      
    
  </div>
  <div class="social-container">
    
      
    
      
    
      
    
      
    
      
    
  </div>
</div>

        <div class="post-detail">
          <article class="post">
            <h2 class="post-title">
              KNN算法及python实现
            </h2>
            <div class="post-info">
              <span>
                2019-07-20
              </span>
              <span>
                1 min read
              </span>
              
                <a href="https://Yz21.github.io//tag/z68jcmpyW" class="post-tag">
                  # 机器学习
                </a>
              
                <a href="https://Yz21.github.io//tag/EWffYRyHc4" class="post-tag">
                  # python
                </a>
              
                <a href="https://Yz21.github.io//tag/Ihx46aKej-" class="post-tag">
                  # KNN
                </a>
              
            </div>
            
            <div class="post-content-wrapper">
              <div class="post-content">
                <meta name="referrer" content="no-referrer" />
<h1 id="前言">前言</h1>
<p>        KNN算法即K-Nearest Neighbor，也是机器学习十大经典算法之一。前文讲解了K-means算法，今天我们就继续讲KNN算法，两者看起来挺相似的，但区别还是很大的，看完本片文章你就会明白了。</p>
<!-- more -->
<h1 id="一-引入">一、引入</h1>
<p>问题：确定绿色圆是属于红色三角形、还是蓝色正方形？<br>
<img src="https://img-blog.csdnimg.cn/20190729110829508.png" alt=""><br>
KNN的思想：<br>
        从上图中我们可以看到，图中的数据集是良好的数据，即都打好了label，一类是蓝色的正方形，一类是红色的三角形，那个绿色的圆形是我们待分类的数据。<br>
        如果K=3，那么离绿色点最近的有2个红色三角形和1个蓝色的正方形，这3个点投票，于是绿色的这个待分类点属于红色的三角形<br>
        如果K=5，那么离绿色点最近的有2个红色三角形和3个蓝色的正方形，这5个点投票，于是绿色的这个待分类点属于蓝色的正方形<br>
        即如果一个样本在特征空间中的k个最相邻的样本中，大多数属于某一个类别，则该样本也属于这个类别。我们可以看到，KNN本质是基于一种数据统计的方法！其实很多机器学习算法也是基于数据统计的。</p>
<h1 id="二-knn算法">二、KNN算法</h1>
<h2 id="1介绍">1.介绍</h2>
<p>        KNN即K-最近邻分类算法（K-Nearest Neighbor），是一种memory-based learning，也叫instance-based learning，属于lazy learning。即它没有明显的前期训练过程，而是程序开始运行时，把数据集加载到内存后，不需要进行训练，就可以开始分类了。<br>
        KNN也是一种监督学习算法，通过计算新数据与训练数据特征值之间的距离，然后选取K(K&gt;=1)个距离最近的邻居进行分类判(投票法)或者回归。若K=1，新数据被简单分配给其近邻的类。</p>
<h2 id="2步骤">2.步骤</h2>
<p>1）计算测试数据与各个训练数据之间的距离；</p>
<blockquote>
<p>(计算距离的方式前文讲k-means时说过，不清楚的可以去查看以下➡<a href="https://hpu-yz.github.io/2019/07/19/K-means%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%E5%8F%8Apython%E5%AE%9E%E7%8E%B0/">传送门</a>)</p>
</blockquote>
<p>2）按照距离的递增关系进行排序；</p>
<p>3）选取距离最小的K个点；</p>
<blockquote>
<p>K值是由自己来确定的</p>
</blockquote>
<p>4）确定前K个点所在类别的出现频率；</p>
<p>5）返回前K个点中出现频率最高的类别作为测试数据的预测分类。</p>
<blockquote>
<p>说明：对于步骤5的预测分类有以下两种方法</p>
<ol>
<li>多数表决法：多数表决法类似于投票的过程，也就是在 K 个邻居中选择类别最多的种类作为测试样本的类别。</li>
<li>加权表决法：根据距离的远近，对近邻的投票进行加权，距离越近则权重越大，通过权重计算结果最大值的类为测试样本的类别。</li>
</ol>
</blockquote>
<h2 id="特点">特点</h2>
<ol>
<li>非参数统计方法：不需要引入参数</li>
<li>K的选择：<br>
        K = 1时，将待分类样本划入与其最接近的样本的类。<br>
        K = |X|时，仅根据训练样本进行频率统计，将待分类样本划入最多的类。<br>
K需要合理选择，太小容易受干扰，太大增加计算复杂性。</li>
<li>算法的复杂度：维度灾难，当维数增加时，所需的训练样本数急剧增加，一般采用降维处理。</li>
</ol>
<h1 id="三-算法优缺点">三、算法优缺点</h1>
<h2 id="优点">优点</h2>
<ol>
<li>简单、有效。</li>
<li>重新训练的代价较低(类别体系的变化和训练集的变化，在Web环境和电子商务应用中是很常见的)。</li>
<li>计算时间和空间线性于训练集的规模(在一些场合不算太大)。</li>
<li>由于KNN方法主要靠周围有限的邻近的样本，而不是靠判别类域的方法来确定所属类别的，因此对于类域的交叉或重叠较多的待分样本集来说，KNN方法较其他方法更为适合。</li>
<li>该算法比较适用于样本容量比较大的类域的自动分类，而那些样本容量较小的类域采用这种算法比较容易产生误分。</li>
</ol>
<h2 id="缺点">缺点</h2>
<ol>
<li>KNN算法是懒散学习方法(lazy learning)，而一些积极学习的算法要快很多。</li>
<li>需要存储全部的训练样本</li>
<li>输出的可解释性不强，例如决策树的可解释性较强。</li>
<li>该算法在分类时有个主要的不足是，当样本不平衡时，如一个类的样本容量很大，而其他类样本容量很小时，有可能导致当输入一个新样本时，该样本的K个邻居中大容量类的样本占多数。该算法只计算最近的邻居样本，某一类的样本数量很大，那么或者这类样本并不接近目标样本，或者这类样本很靠近目标样本。无论怎样，数量并不能影响运行结果。可以采用权值的方法(和该样本距离小的邻居权值大)来改进。</li>
<li>计算量较大。目前常用的解决方法是事先对已知样本点进行剪辑，事先去除对分类作用不大的样本。</li>
</ol>
<h1 id="四-knn与k-means的区别">四、KNN与K-means的区别</h1>
<p>        废话不多说，咱直接上图：<br>
<img src="https://img-blog.csdnimg.cn/2019072911370194.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzQxMzEy,size_16,color_FFFFFF,t_70" alt=""><br>
相似点：<br>
        虽然两者有很大且别，但两者也有共同之处。都包含了一个过程：给定一个点，在数据集找离它最近的点，即都用到了NN(Nearest Neighbor)算法。</p>
<h1 id="五-python实例实现">五、python实例实现</h1>
<p>        下面引入一个实例，通过python代码具体看下KNN算法的流程。</p>
<pre><code>from numpy import *
import operator

dataSet = array([[1.0,1.1],[1.0,1.0],[0,0],[0,0.1]])
labels = ['A','A','B','B']

def classify0(inX,dataSet,labels,k):

    #求出样本集的行数，也就是labels标签的数目
    dataSetSize = dataSet.shape[0]

    #构造输入值和样本集的差值矩阵
    diffMat = tile(inX,(dataSetSize,1)) - dataSet

    #计算欧式距离
    sqDiffMat = diffMat**2
    sqDistances = sqDiffMat.sum(axis=1)
    distances = sqDistances**0.5

    #求距离从小到大排序的序号
    sortedDistIndicies = distances.argsort()

    #对距离最小的k个点统计对应的样本标签
    classCount = {}
    for i in range(k):
        #取第i+1邻近的样本对应的类别标签
        voteIlabel = labels[sortedDistIndicies[i]]
        #以标签为key，标签出现的次数为value将统计到的标签及出现次数写进字典
        classCount[voteIlabel] = classCount.get(voteIlabel,0) + 1

    #对字典按value从大到小排序
    sortedClassCount = sorted(classCount.items(),key=operator.itemgetter(1),reverse=True)

    #返回排序后字典中最大value对应的key
    return sortedClassCount[0][0]
if __name__ == '__main__':
    print(classify0([1.1,0],dataSet,labels,3))
</code></pre>

              </div>
              <div class="toc-container">
                <ul class="markdownIt-TOC">
<li><a href="#%E5%89%8D%E8%A8%80">前言</a></li>
<li><a href="#%E4%B8%80-%E5%BC%95%E5%85%A5">一、引入</a></li>
<li><a href="#%E4%BA%8C-knn%E7%AE%97%E6%B3%95">二、KNN算法</a>
<ul>
<li><a href="#1%E4%BB%8B%E7%BB%8D">1.介绍</a></li>
<li><a href="#2%E6%AD%A5%E9%AA%A4">2.步骤</a></li>
<li><a href="#%E7%89%B9%E7%82%B9">特点</a></li>
</ul>
</li>
<li><a href="#%E4%B8%89-%E7%AE%97%E6%B3%95%E4%BC%98%E7%BC%BA%E7%82%B9">三、算法优缺点</a>
<ul>
<li><a href="#%E4%BC%98%E7%82%B9">优点</a></li>
<li><a href="#%E7%BC%BA%E7%82%B9">缺点</a></li>
</ul>
</li>
<li><a href="#%E5%9B%9B-knn%E4%B8%8Ek-means%E7%9A%84%E5%8C%BA%E5%88%AB">四、KNN与K-means的区别</a></li>
<li><a href="#%E4%BA%94-python%E5%AE%9E%E4%BE%8B%E5%AE%9E%E7%8E%B0">五、python实例实现</a></li>
</ul>

              </div>
            </div>
          </article>
        </div>

        
          <div class="next-post">
            <div class="next">下一篇</div>
            <a href="https://Yz21.github.io//post/K-means聚类算法原理及python实现">
              <h3 class="post-title">
                K-means算法及python实现
              </h3>
            </a>
          </div>
        

        

        <div class="site-footer">
  Powered by <a href="https://github.com/getgridea/gridea" target="_blank">Gridea</a> | 
  <a class="rss" href="https://Yz21.github.io//atom.xml" target="_blank">RSS</a>
</div>

<script>
  hljs.initHighlightingOnLoad()

  let mainNavLinks = document.querySelectorAll(".markdownIt-TOC a");

  // This should probably be throttled.
  // Especially because it triggers during smooth scrolling.
  // https://lodash.com/docs/4.17.10#throttle
  // You could do like...
  // window.addEventListener("scroll", () => {
  //    _.throttle(doThatStuff, 100);
  // });
  // Only not doing it here to keep this Pen dependency-free.

  window.addEventListener("scroll", event => {
    let fromTop = window.scrollY;

    mainNavLinks.forEach((link, index) => {
      let section = document.getElementById(decodeURI(link.hash).substring(1));
      let nextSection = null
      if (mainNavLinks[index + 1]) {
        nextSection = document.getElementById(decodeURI(mainNavLinks[index + 1].hash).substring(1));
      }
      console.log('section.offsetHeight', section.offsetHeight);
      if (section.offsetTop <= fromTop) {
        if (nextSection) {
          if (nextSection.offsetTop > fromTop) {
            link.classList.add("current");
          } else {
            link.classList.remove("current");    
          }
        } else {
          link.classList.add("current");
        }
      } else {
        link.classList.remove("current");
      }
    });
  });

</script>

      </div>
    </div>
  </body>
</html>
