<html>
  <head>
    <meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>K-means算法及python实现 | Gridea</title>
<meta name="description" content="温故而知新" />
<link rel="shortcut icon" href="https://Yz21.github.io//favicon.ico?v=1565658376063">
<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.7.2/css/all.css" integrity="sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css">
<link rel="stylesheet" href="https://Yz21.github.io//styles/main.css">

<script src="https://cdn.bootcss.com/highlight.js/9.12.0/highlight.min.js"></script>
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Droid+Serif:400,700">



  </head>
  <body>
    <div class="main">
      <div class="main-content">
        <div class="site-header">
  <a href="https://Yz21.github.io/">
  <img class="avatar" src="https://Yz21.github.io//images/avatar.png?v=1565658376063" alt="">
  </a>
  <h1 class="site-title">
    Gridea
  </h1>
  <p class="site-description">
    温故而知新
  </p>
  <div class="menu-container">
    
      
        <a href="/" class="menu">
          首页
        </a>
      
    
      
        <a href="/archives" class="menu">
          归档
        </a>
      
    
      
        <a href="/tags" class="menu">
          标签
        </a>
      
    
      
        <a href="/post/about" class="menu">
          关于
        </a>
      
    
  </div>
  <div class="social-container">
    
      
    
      
    
      
    
      
    
      
    
  </div>
</div>

        <div class="post-detail">
          <article class="post">
            <h2 class="post-title">
              K-means算法及python实现
            </h2>
            <div class="post-info">
              <span>
                2019-07-19
              </span>
              <span>
                2 min read
              </span>
              
                <a href="https://Yz21.github.io//tag/z68jcmpyW" class="post-tag">
                  # 机器学习
                </a>
              
                <a href="https://Yz21.github.io//tag/EWffYRyHc4" class="post-tag">
                  # python
                </a>
              
                <a href="https://Yz21.github.io//tag/AShOqXWA95" class="post-tag">
                  # K-means
                </a>
              
            </div>
            
            <div class="post-content-wrapper">
              <div class="post-content">
                <meta name="referrer" content="no-referrer" />
<h1 id="前言">前言</h1>
<p>        K-means(Thek-meansalgorithm)是机器学习十大经典算法之一，同时也是最为经典的无监督聚类（Unsupervised Clustering）算法。接触聚类算法，首先需要了解k-means算法的实现原理和步骤。本文将对k-means算法的基本原理和实现实例进行分析。</p>
<!-- more -->
<h1 id="一聚类算法的简介">一.聚类算法的简介</h1>
<p>        对于&quot;<strong>监督学习</strong>&quot;(supervised learning)，其训练样本<strong>是带有标记信息</strong>的，并且监督学习的<strong>目的是</strong>：对带有标记的数据集进行模型学习，从而便于对新的样本进行分类。而在“<strong>无监督学习</strong>”(unsupervised learning)中，训练样本的标记信息是未知的，<strong>目标是</strong>通过对无标记训练样本的学习来揭示数据的内在性质及规律，为进一步的数据分析提供基础。对于无监督学习，应用最广的便是&quot;<strong>聚类</strong>&quot;(clustering)。<br>
        &quot;<strong>聚类算法</strong>&quot;试图将数据集中的样本划分为若干个通常是不相交的子集，每个子集称为一个“簇”(cluster)，通过这样的划分，每个簇可能对应于一些潜在的概念或类别。<br>
        我们可以通过下面这个图来理解：<br>
                                                                                  <img src="https://img-blog.csdnimg.cn/20190724144117607.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzQxMzEy,size_16,color_FFFFFF,t_70" alt=""><br>
        上图是未做标记的样本集，通过他们的分布，我们很容易对上图中的样本做出以下几种划分。<br>
                当需要将其划分为两个簇时，即 k=2时：<br>
<img src="https://img-blog.csdnimg.cn/20190726155350351.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzQxMzEy,size_16,color_FFFFFF,t_70" alt=""><br>
        当需要将其划分为四个簇时，即 k=4 时：<br>
                                                                                <img src="https://img-blog.csdnimg.cn/20190724144431802.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzQxMzEy,size_16,color_FFFFFF,t_70" alt=""></p>
<h1 id="二k-means聚类算法">二.K-means聚类算法</h1>
<p>        kmeans算法又名k均值算法,K-means算法中的k表示的是聚类为k个簇，means代表取每一个聚类中数据值的均值作为该簇的中心，或者称为质心，即用每一个的类的质心对该簇进行描述。<br>
        其算法思想大致为：先从样本集中随机选取 k个样本作为簇中心，并计算所有样本与这 k个“簇中心”的距离，对于每一个样本，将其划分到与其距离最近的“簇中心”所在的簇中，对于新的簇计算各个簇的新的“簇中心”。<br>
        根据以上描述，我们大致可以猜测到实现kmeans算法的主要四点：<br>
          （1）簇个数 k 的选择<br>
          （2）各个样本点到“簇中心”的距离<br>
          （3）根据新划分的簇，更新“簇中心”<br>
          （4）重复上述2、3过程，直至&quot;簇中心&quot;没有移动<br>
        优缺点：</p>
<ul>
<li>优点：容易实现</li>
<li>缺点：可能收敛到局部最小值，在大规模数据上收敛较慢</li>
</ul>
<h1 id="三k-means算法步骤详解">三.K-means算法步骤详解</h1>
<h2 id="step1k值的选择">Step1.K值的选择</h2>
<p>k 的选择一般是按照实际需求进行决定，或在实现算法时直接给定 k 值。</p>
<blockquote>
<p>说明：<br>
<strong>A</strong>.质心数量由用户给出，记为k，k-means最终得到的簇数量也是k<br>
<strong>B</strong>.后来每次更新的质心的个数都和初始k值相等<br>
<strong>C</strong>.k-means最后聚类的簇个数和用户指定的质心个数相等，一个质心对应一个簇，每个样本只聚类到一个簇里面<br>
<strong>D</strong>.初始簇为空</p>
</blockquote>
<h2 id="step2距离度量">Step2.距离度量</h2>
<p>        将对象点分到距离聚类中心最近的那个簇中需要最近邻的度量策略，在欧式空间中采用的是欧式距离，在处理文档中采用的是余弦相似度函数，有时候也采用曼哈顿距离作为度量，不同的情况实用的度量公式是不同的。</p>
<h3 id="21欧式距离">2.1.欧式距离</h3>
<p><img src="https://img-blog.csdnimg.cn/20190724141524460.png" alt=""></p>
<h3 id="22曼哈顿距离">2.2.曼哈顿距离</h3>
<p><img src="https://img-blog.csdnimg.cn/20190724141601149.png" alt=""></p>
<h3 id="23余弦相似度">2.3.余弦相似度</h3>
<p>        A与B表示向量(x1,y1)，(x2,y2)<br>
        分子为A与B的点乘，分母为二者各自的L2相乘，即将所有维度值的平方相加后开方。<br>
<img src="https://img-blog.csdnimg.cn/2019072611455719.png" alt=""></p>
<blockquote>
<p>说明：<br>
<strong>A</strong>.经过step2，得到k个新的簇，每个样本都被分到k个簇中的某一个簇<br>
<strong>B</strong>.得到k个新的簇后，当前的质心就会失效，需要计算每个新簇的自己的新质心</p>
</blockquote>
<h2 id="step3新质心的计算">Step3.新质心的计算</h2>
<p>        对于分类后的产生的k个簇，分别计算到簇内其他点距离均值最小的点作为质心（对于拥有坐标的簇可以计算每个簇坐标的均值作为质心）</p>
<blockquote>
<p>说明：<br>
<strong>A</strong>.比如一个新簇有3个样本：[[1,4], [2,5], [3,6]]，得到此簇的新质心=[(1+2+3)/3,   (4+5+6)/3]<br>
<strong>B</strong>.经过step3，会得到k个新的质心，作为step2中使用的质心</p>
</blockquote>
<h2 id="step4是否停止k-means">Step4.是否停止K-means</h2>
<p>        质心不再改变，或给定loop最大次数loopLimit</p>
<blockquote>
<p>说明：<br>
<strong>A</strong>当每个簇的质心，不再改变时就可以停止k-menas<br>
<strong>B</strong>.当loop次数超过looLimit时，停止k-means<br>
<strong>C</strong>.只需要满足两者的其中一个条件，就可以停止k-means<br>
<strong>C</strong>.如果Step4没有结束k-means，就再执行step2-step3-step4<br>
<strong>D</strong>.如果Step4结束了k-means，则就打印(或绘制)簇以及质心</p>
</blockquote>
<h1 id="四python实现代码详解">四.python实现+代码详解</h1>
<p>        以下是python得实例代码以及代码的详解，应该可以理解的。</p>
<pre><code>import random
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# 计算欧拉距离
def calcDis(dataSet, centroids, k):
    clalist=[]
    for data in dataSet:
        diff = np.tile(data, (k, 1)) - centroids  #相减   (np.tile(a,(2,1))就是把a先沿x轴复制1倍，即没有复制，仍然是 [0,1,2]。 再把结果沿y方向复制2倍得到array([[0,1,2],[0,1,2]]))
        squaredDiff = diff ** 2     #平方
        squaredDist = np.sum(squaredDiff, axis=1)   #和  (axis=1表示行)
        distance = squaredDist ** 0.5  #开根号
        clalist.append(distance) 
    clalist = np.array(clalist)  #返回一个每个点到质点的距离len(dateSet)*k的数组
    return clalist

# 计算质心
def classify(dataSet, centroids, k):
    # 计算样本到质心的距离
    clalist = calcDis(dataSet, centroids, k)
    # 分组并计算新的质心
    minDistIndices = np.argmin(clalist, axis=1)    #axis=1 表示求出每行的最小值的下标
    newCentroids = pd.DataFrame(dataSet).groupby(minDistIndices).mean() #DataFramte(dataSet)对DataSet分组，groupby(min)按照min进行统计分类，mean()对分类结果求均值
    newCentroids = newCentroids.values
 
    # 计算变化量
    changed = newCentroids - centroids
 
    return changed, newCentroids

# 使用k-means分类
def kmeans(dataSet, k):
    # 随机取质心
    centroids = random.sample(dataSet, k)
    
    # 更新质心 直到变化量全为0
    changed, newCentroids = classify(dataSet, centroids, k)
    while np.any(changed != 0):
        changed, newCentroids = classify(dataSet, newCentroids, k)
 
    centroids = sorted(newCentroids.tolist())   #tolist()将矩阵转换成列表 sorted()排序
 
    # 根据质心计算每个集群
    cluster = []
    clalist = calcDis(dataSet, centroids, k) #调用欧拉距离
    minDistIndices = np.argmin(clalist, axis=1)  
    for i in range(k):
        cluster.append([])
    for i, j in enumerate(minDistIndices):   #enymerate()可同时遍历索引和遍历元素
        cluster[j].append(dataSet[i])
        
    return centroids, cluster
 
# 创建数据集
def createDataSet():
    return [[1, 1], [1, 2], [2, 1], [6, 4], [6, 3], [5, 4]]

if __name__=='__main__': 
    dataset = createDataSet()
    centroids, cluster = kmeans(dataset, 2)
    print('质心为：%s' % centroids)
    print('集群为：%s' % cluster)
    for i in range(len(dataset)):
      plt.scatter(dataset[i][0],dataset[i][1], marker = 'o',color = 'green', s = 40 ,label = '原始点')
                                                    #  记号形状       颜色      点的大小      设置标签
      for j in range(len(centroids)):
        plt.scatter(centroids[j][0],centroids[j][1],marker='x',color='red',s=50,label='质心')
        plt.show
</code></pre>
<h1 id="五k-means算法补充">五.K-means算法补充</h1>
<p>1.对初始化敏感，初始质点k给定的不同，可能会产生不同的聚类结果。如下图所示，右边是k=2的结果，这个就正好，而左图是k=3的结果，可以看到右上角得这两个簇应该是可以合并成一个簇的。</p>
<p>改进：<br>
对k的选择可以先用一些算法分析数据的分布，如重心和密度等，然后选择合适的k<br>
<img src="https://img-blog.csdnimg.cn/20190726154810167.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzQxMzEy,size_16,color_FFFFFF,t_70" alt=""><br>
2.<strong>使用存在局限性</strong>，如下面这种非球状的数据分布就搞不定了：<br>
<img src="https://img-blog.csdnimg.cn/20190724161221112.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzQxMzEy,size_16,color_FFFFFF,t_70" alt=""><br>
3.数据集比较大的时候，<strong>收敛会比较慢</strong>。</p>
<p>4.最终会收敛。不管初始点如何选择，最终都会收敛。可是是全局收敛，也可能是局部收敛。</p>
<h1 id="六小结">六.小结</h1>
<p>        1. 聚类是一种无监督的学习方法。聚类区别于分类，即事先不知道要寻找的内容，没有预先设定好的目标变量。</p>
<p>        2. 聚类将数据点归到多个簇中，其中相似的数据点归为同一簇，而不相似的点归为不同的簇。相似度的计算方法有很多，具体的应用选择合适的相似度计算方法</p>
<p>        3. K-means聚类算法，是一种广泛使用的聚类算法，其中k是需要指定的参数，即需要创建的簇的数目，K-means算法中的k个簇的质心可以通过随机的方式获得，但是这些点需要位于数据范围内。在算法中，计算每个点到质心得距离，选择距离最小的质心对应的簇作为该数据点的划分，然后再基于该分配过程后更新簇的质心。重复上述过程，直至各个簇的质心不再变化为止。</p>
<p>        4. K-means算法虽然有效，但是容易受到初始簇质心的情况而影响，有可能陷入局部最优解。为了解决这个问题，可以使用另外一种称为二分K-means的聚类算法。二分K-means算法首先将所有数据点分为一个簇；然后使用K-means（k=2）对其进行划分；下一次迭代时，选择使得SSE下降程度最大的簇进行划分；重复该过程，直至簇的个数达到指定的数目为止。实验表明，二分K-means算法的聚类效果要好于普通的K-means聚类算法。</p>

              </div>
              <div class="toc-container">
                <ul class="markdownIt-TOC">
<li><a href="#%E5%89%8D%E8%A8%80">前言</a></li>
<li><a href="#%E4%B8%80%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E7%9A%84%E7%AE%80%E4%BB%8B">一.聚类算法的简介</a></li>
<li><a href="#%E4%BA%8Ck-means%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95">二.K-means聚类算法</a></li>
<li><a href="#%E4%B8%89k-means%E7%AE%97%E6%B3%95%E6%AD%A5%E9%AA%A4%E8%AF%A6%E8%A7%A3">三.K-means算法步骤详解</a>
<ul>
<li><a href="#step1k%E5%80%BC%E7%9A%84%E9%80%89%E6%8B%A9">Step1.K值的选择</a></li>
<li><a href="#step2%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F">Step2.距离度量</a>
<ul>
<li><a href="#21%E6%AC%A7%E5%BC%8F%E8%B7%9D%E7%A6%BB">2.1.欧式距离</a></li>
<li><a href="#22%E6%9B%BC%E5%93%88%E9%A1%BF%E8%B7%9D%E7%A6%BB">2.2.曼哈顿距离</a></li>
<li><a href="#23%E4%BD%99%E5%BC%A6%E7%9B%B8%E4%BC%BC%E5%BA%A6">2.3.余弦相似度</a></li>
</ul>
</li>
<li><a href="#step3%E6%96%B0%E8%B4%A8%E5%BF%83%E7%9A%84%E8%AE%A1%E7%AE%97">Step3.新质心的计算</a></li>
<li><a href="#step4%E6%98%AF%E5%90%A6%E5%81%9C%E6%AD%A2k-means">Step4.是否停止K-means</a></li>
</ul>
</li>
<li><a href="#%E5%9B%9Bpython%E5%AE%9E%E7%8E%B0%E4%BB%A3%E7%A0%81%E8%AF%A6%E8%A7%A3">四.python实现+代码详解</a></li>
<li><a href="#%E4%BA%94k-means%E7%AE%97%E6%B3%95%E8%A1%A5%E5%85%85">五.K-means算法补充</a></li>
<li><a href="#%E5%85%AD%E5%B0%8F%E7%BB%93">六.小结</a></li>
</ul>

              </div>
            </div>
          </article>
        </div>

        
          <div class="next-post">
            <div class="next">下一篇</div>
            <a href="https://Yz21.github.io//post/数据集的划分--训练集、验证集和测试集">
              <h3 class="post-title">
                数据集的划分--训练集、验证集和测试集
              </h3>
            </a>
          </div>
        

        

        <div class="site-footer">
  Powered by <a href="https://github.com/getgridea/gridea" target="_blank">Gridea</a> | 
  <a class="rss" href="https://Yz21.github.io//atom.xml" target="_blank">RSS</a>
</div>

<script>
  hljs.initHighlightingOnLoad()

  let mainNavLinks = document.querySelectorAll(".markdownIt-TOC a");

  // This should probably be throttled.
  // Especially because it triggers during smooth scrolling.
  // https://lodash.com/docs/4.17.10#throttle
  // You could do like...
  // window.addEventListener("scroll", () => {
  //    _.throttle(doThatStuff, 100);
  // });
  // Only not doing it here to keep this Pen dependency-free.

  window.addEventListener("scroll", event => {
    let fromTop = window.scrollY;

    mainNavLinks.forEach((link, index) => {
      let section = document.getElementById(decodeURI(link.hash).substring(1));
      let nextSection = null
      if (mainNavLinks[index + 1]) {
        nextSection = document.getElementById(decodeURI(mainNavLinks[index + 1].hash).substring(1));
      }
      console.log('section.offsetHeight', section.offsetHeight);
      if (section.offsetTop <= fromTop) {
        if (nextSection) {
          if (nextSection.offsetTop > fromTop) {
            link.classList.add("current");
          } else {
            link.classList.remove("current");    
          }
        } else {
          link.classList.add("current");
        }
      } else {
        link.classList.remove("current");
      }
    });
  });

</script>

      </div>
    </div>
  </body>
</html>
